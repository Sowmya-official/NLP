{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-summarization using functions for all reading input text.ipynb",
      "provenance": [],
      "mount_file_id": "1m7IRlyvKRTuWA93RickNFnHjf23mO9C3",
      "authorship_tag": "ABX9TyPLkRYoi5aGx1Cd7a3JyZgG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sowmya-official/NLP/blob/main/text_summarization_using_functions_for_all_reading_input_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install PyPDF2"
      ],
      "metadata": {
        "id": "oJgrvOyf4gt6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ur63bs7o37dT"
      },
      "outputs": [],
      "source": [
        "#Step 1. Importing Libraries\n",
        "\n",
        "import sys\n",
        "import math\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this line if you are running this code for first time\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Initializing few variable\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrc1J9pl4OZg",
        "outputId": "ee32e943-bddd-41e7-9381-93df18e4108e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2. Define functions for Reading Input Text\n",
        "\n",
        "#Function to Read .txt File and return its Text\n",
        "def file_text(filepath):\n",
        "    with open(filepath) as f:\n",
        "        text = f.read().replace(\"\\n\", '')\n",
        "        return text\n",
        "\n",
        "#Function to Read PDF File and return its Text\n",
        "def pdfReader(pdf_path):\n",
        "    \n",
        "    with open(pdf_path, 'rb') as pdfFileObject:\n",
        "        pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
        "        count = pdfReader.numPages\n",
        "        print(\"\\nTotal Pages in pdf = \", count)\n",
        "        \n",
        "        c = 'Y'\n",
        "        start_page = 0\n",
        "        end_page = count-1\n",
        "        c = input(\"Do you want to read entire pdf ?[Y]/N  :  \")\n",
        "        if c == 'N' or c == 'n' :\n",
        "            start_page  = int(input(\"Enter start page number (Indexing start from 0) :  \"))\n",
        "            end_page = int(input(f\"Enter end page number (Less than {count}) : \"))\n",
        "            \n",
        "            if start_page <0 or start_page >= count:\n",
        "                print(\"\\nInvalid Start page given\")\n",
        "                sys.exit()\n",
        "                \n",
        "            if end_page <0 or end_page >= count:\n",
        "                print(\"\\nInvalid End page given\")\n",
        "                sys.exit()\n",
        "                \n",
        "        for i in range(start_page,end_page+1):\n",
        "            page = pdfReader.getPage(i)\n",
        "\n",
        "        return page.extractText()\n",
        "\n",
        "\n",
        "#Function to Read wikipedia page url and return its Text   \n",
        "def wiki_text(url):\n",
        "    scrap_data = urllib.request.urlopen(url)\n",
        "    article = scrap_data.read()\n",
        "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "    \n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    article_text = \"\"\n",
        "    \n",
        "    for p in paragraphs:\n",
        "        article_text += p.text\n",
        "    \n",
        "    #Removing all unwanted characters\n",
        "    article_text = re.sub(r'\\[[0-9]*\\]', '', article_text)\n",
        "    return article_text\n"
      ],
      "metadata": {
        "id": "QO6pxbGW4rA6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8oi0S5Uk5jgH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3. Getting Text \n",
        "\n",
        "input_text_type = int(input(\"Select one way of inputting your text  \\\n",
        ": \\n1. Type your Text(or Copy-Paste)\\n2. Load from .txt file\\n3. Load from .pdf file\\n4. From Wikipedia Page URL\\n\\n\"))\n",
        "\n",
        "if input_text_type == 1:\n",
        "    text = input(u\"Enter your text : \\n\\n\")\n",
        "\n",
        "elif input_text_type == 2:\n",
        "    txt_path = input(\"Enter file path :  \")\n",
        "    text = file_text(txt_path)\n",
        "    \n",
        "        \n",
        "elif input_text_type == 3:\n",
        "    file_path = input(\"Enter file path :  \")\n",
        "    text = pdfReader(file_path)\n",
        "    \n",
        "elif input_text_type == 4:\n",
        "    wiki_url = input(\"Enter Wikipedia URL to load Article : \")\n",
        "    text = wiki_text(wiki_url)\n",
        "    \n",
        "else:\n",
        "    print(\"Sorry! Wrong Input, Try Again.\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-W0BEFx5VX9",
        "outputId": "9c28bdcb-58a3-410b-ae2d-6dd5e09933ad"
      },
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select one way of inputting your text  : \n",
            "1. Type your Text(or Copy-Paste)\n",
            "2. Load from .txt file\n",
            "3. Load from .pdf file\n",
            "4. From Wikipedia Page URL\n",
            "\n",
            "1\n",
            "Enter your text : \n",
            "\n",
            "Select one way of inputting your text  :  1. Type your Text(or Copy-Paste) 2. Load from .txt file 3. Load from .pdf file 4. From Wikipedia Page URL  1 Enter your text :   Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning and focuses on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4. Defining functions to create Tf-Idf Matrix\n",
        "\n",
        "\n",
        "#Function to calculate frequency of word in each sentence\n",
        "#INPUT -> List of all sentences from text as spacy.Doc object\n",
        "#OUTPUT -> freq_matrix (A dictionary with each sentence itself as key, \n",
        "# and a dictionary of words of that sentence with their frequency as value)\n",
        "\n",
        "def frequency_matrix(sentences):\n",
        "    freq_matrix = {}\n",
        "    stopWords = nlp.Defaults.stop_words\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {} #dictionary with 'words' as key and their 'frequency' as value\n",
        "        \n",
        "        #Getting all word from the sentence in lower case\n",
        "        words = [word.text.lower() for word in sent  if word.text.isalnum()]\n",
        "       \n",
        "        for word in words:  \n",
        "            word = lemmatizer.lemmatize(word)   #Lemmatize the word\n",
        "            if word not in stopWords:           #Reject stopwords\n",
        "                if word in freq_table:\n",
        "                    freq_table[word] += 1\n",
        "                else:\n",
        "                    freq_table[word] = 1\n",
        "\n",
        "        freq_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return freq_matrix\n"
      ],
      "metadata": {
        "id": "yW_dKEucHzFj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to calculate Term Frequency(TF) of each word\n",
        "#INPUT -> freq_matrix\n",
        "#OUTPUT -> tf_matrix (A dictionary with each sentence itself as key, \n",
        "# and a dictionary of words of that sentence with their Term-Frequency as value)\n",
        "\n",
        "#TF(t) = (Number of times term t appears in  document) / (Total number of terms in the document)\n",
        "def tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, freq_table in freq_matrix.items():\n",
        "        tf_table = {}  #dictionary with 'word' itself as a key and its TF as value\n",
        "\n",
        "        total_words_in_sentence = len(freq_table)\n",
        "        for word, count in freq_table.items():\n",
        "            tf_table[word] = count / total_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix"
      ],
      "metadata": {
        "id": "ybILtO7QIOwK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to find how many sentences contain a 'word'\n",
        "#INPUT -> freq_matrix\n",
        "#OUTPUT -> sent_per_words (Dictionary with each word itself as key and number of \n",
        "#sentences containing that word as value)\n",
        "\n",
        "def sentences_per_words(freq_matrix):\n",
        "    sent_per_words = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in sent_per_words:\n",
        "                sent_per_words[word] += 1\n",
        "            else:\n",
        "                sent_per_words[word] = 1\n",
        "\n",
        "    return sent_per_words\n"
      ],
      "metadata": {
        "id": "f697kKtVIRIN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to calculate Inverse Document frequency(IDF) for each word\n",
        "#INPUT -> freq_matrix,sent_per_words, total_sentences\n",
        "#OUTPUT -> idf_matrix (A dictionary with each sentence itself as key, \n",
        "# and a dictionary of words of that sentence with their IDF as value)\n",
        "\n",
        "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "def idf_matrix(freq_matrix, sent_per_words, total_sentences):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_sentences / float(sent_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix"
      ],
      "metadata": {
        "id": "nioKXvFYITqk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to calculate Tf-Idf score of each word\n",
        "#INPUT -> tf_matrix, idf_matrix\n",
        "#OUTPUT - > tf_idf_matrix (A dictionary with each sentence itself as key, \n",
        "# and a dictionary of words of that sentence with their Tf-Idf as value)\n",
        "def tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "       #word1 and word2 are same\n",
        "        for (word1, tf_value), (word2, idf_value) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  \n",
        "            tf_idf_table[word1] = float(tf_value * idf_value)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n"
      ],
      "metadata": {
        "id": "jxZSN9WvIVzV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to rate every sentence with some score calculated on basis of Tf-Idf\n",
        "#INPUT -> tf_idf_matrix\n",
        "#OUTPUT - > sentenceScore (Dictionary with each sentence itself as key and its score\n",
        "# as value)\n",
        "def score_sentences(tf_idf_matrix):\n",
        "    \n",
        "    sentenceScore = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_tfidf_score_per_sentence = 0\n",
        "\n",
        "        total_words_in_sentence = len(f_table)\n",
        "        for word, tf_idf_score in f_table.items():\n",
        "            total_tfidf_score_per_sentence += tf_idf_score\n",
        "\n",
        "        if total_words_in_sentence != 0:\n",
        "            sentenceScore[sent] = total_tfidf_score_per_sentence / total_words_in_sentence\n",
        "\n",
        "    return sentenceScore\n"
      ],
      "metadata": {
        "id": "71J2a0nvIYWq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function Calculating average sentence score \n",
        "#INPUT -> sentence_score\n",
        "#OUTPUT -> average_sent_score(An average of the sentence_score) \n",
        "def average_score(sentence_score):\n",
        "    \n",
        "    total_score = 0\n",
        "    for sent in sentence_score:\n",
        "        total_score += sentence_score[sent]\n",
        "\n",
        "    average_sent_score = (total_score / len(sentence_score))\n",
        "\n",
        "    return average_sent_score"
      ],
      "metadata": {
        "id": "22PVVIosIaqk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to return summary of article\n",
        "#INPUT -> sentences(list of all sentences in article), sentence_score, threshold\n",
        "# (set to the average pf sentence_score)\n",
        "#OUTPUT -> summary (String text)\n",
        "def create_summary(sentences, sentence_score, threshold):\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentence_score and sentence_score[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence.text\n",
        "        \n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "X8lhU17QIdPE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "PzefHHDVI5jk"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5. Using all functions to generate summary\n",
        "\n",
        "\n",
        "#Counting number of words in original article\n",
        "\n",
        "original_words = text.split()\n",
        "original_words = [w for w in original_words if w.isalnum()]\n",
        "num_words_in_original_text = len(original_words)\n",
        "\n",
        "\n",
        "#Converting received text into sapcy Doc object\n",
        "text = nlp(text)\n",
        "\n",
        "#Extracting all sentences from the text in a list\n",
        "sentences = list(text.sents)\n",
        "total_sentences = len(sentences)\n",
        "\n",
        "#Generating Frequency Matrix\n",
        "freq_matrix = frequency_matrix(sentences)\n",
        "\n",
        "#Generating Term Frequency Matrix\n",
        "tf_matrix = tf_matrix(freq_matrix)\n",
        "\n",
        "#Getting number of sentences containing a particular word\n",
        "num_sent_per_words = sentences_per_words(freq_matrix)\n",
        "\n",
        "#Generating ID Frequency Matrix\n",
        "idf_matrix = idf_matrix(freq_matrix, num_sent_per_words, total_sentences)\n",
        "\n",
        "#Generating Tf-Idf Matrix\n",
        "tf_idf_matrix = tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "\n",
        "\n",
        "#Generating Sentence score for each sentence\n",
        "sentence_scores = score_sentences(tf_idf_matrix)\n",
        "\n",
        "#Setting threshold to average value (You are free to play with ther values) \n",
        "threshold = average_score(sentence_scores)\n",
        "\n",
        "#Getting summary \n",
        "summary = create_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "print(\"\\n\\n\")\n",
        "print(\"*\"*20,\"Summary\",\"*\"*20)\n",
        "print(\"\\n\")\n",
        "print(summary)\n",
        "print(\"\\n\\n\")\n",
        "print(\"Total words in original article = \", num_words_in_original_text)\n",
        "print(\"Total words in summarized article = \", len(summary.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w4SlUWiIfoE",
        "outputId": "fd5e36fc-dbd5-4fa2-f5e7-40f65e6a9c50"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "******************** Summary ********************\n",
            "\n",
            "\n",
            " Load from .txt file 3. Load from .pdf file 4.\n",
            "\n",
            "\n",
            "\n",
            "Total words in original article =  159\n",
            "Total words in summarized article =  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "--X8a0YIIjpO"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}